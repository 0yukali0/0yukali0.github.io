"use strict";(self.webpackChunk_0_yukali_0_github_io=self.webpackChunk_0_yukali_0_github_io||[]).push([[8218],{237:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var t=r(4848),a=r(8453);const i={id:"ray_train_framework",title:"Ray Train\u652f\u63f4\u4e4bframework"},o="Ray train\u652f\u6301\u4e4bframerwork",s={id:"kubernetes-native/mlops/ray/train/ray_train_framework",title:"Ray Train\u652f\u63f4\u4e4bframework",description:"Pytorch",source:"@site/docs/kubernetes-native/mlops/ray/train/train_framework.md",sourceDirName:"kubernetes-native/mlops/ray/train",slug:"/kubernetes-native/mlops/ray/train/ray_train_framework",permalink:"/docs/kubernetes-native/mlops/ray/train/ray_train_framework",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/kubernetes-native/mlops/ray/train/train_framework.md",tags:[],version:"current",frontMatter:{id:"ray_train_framework",title:"Ray Train\u652f\u63f4\u4e4bframework"}},l={},d=[{value:"Pytorch",id:"pytorch",level:2},{value:"\u5b58\u53d6ray model\u4e4b\u8a13\u7df4\u8cc7\u8a0a",id:"\u5b58\u53d6ray-model\u4e4b\u8a13\u7df4\u8cc7\u8a0a",level:3},{value:"Pytorch lightning",id:"pytorch-lightning",level:2},{value:"Hugging Face Transformers",id:"hugging-face-transformers",level:2},{value:"Hugging Face Accelerate",id:"hugging-face-accelerate",level:2},{value:"DeepSpeed",id:"deepspeed",level:2},{value:"Tensorflow &amp;  Keras",id:"tensorflow---keras",level:2},{value:"XGBoost &amp; LightGBM",id:"xgboost--lightgbm",level:2},{value:"Horovod",id:"horovod",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"ray-train\u652f\u6301\u4e4bframerwork",children:"Ray train\u652f\u6301\u4e4bframerwork"})}),"\n",(0,t.jsx)(e.h2,{id:"pytorch",children:"Pytorch"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import tempfile\n\nfrom torchvision.models import resnet18\nfrom torchvision.dataset import FashionMNIST\nfrom torchvision.transform import toTensor, Normalize, Compose\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\n\n# Model\nfunc build_model(distributed_model=None):\n    model = restnet18(num_classes=10)\n    model.conv1 = torch.nn.Conv2d( 1, 64, kernel_size(7,7), stride=(2,2), padding=(3,3), bias=False)\n    criterion = CrossEntropyLoss()\n    if distributed_model is not None:\n        model = distributed_model(model)\n    optimizer = Adam(model.parameters(), lr=0.001)\n    return model, criterion, optimizer\n\n\n# Data\nfunc build_dataloader(distributed_loader=None):\n    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n    train_data = FashionMNIST(\'./data\', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n    if distributed_loader is not None:\n        train_loader = distributed_loader(train_loader)\n    return train_loader\n\n# Training\nfunc training(get_model=None, get_data=None, distributed_report=None, checkpoint_func=None)\n    model, criterion, optimizer = build_model()\n    train_loader = build_dataloader()\n\n    if get_model is not None:\n        model, criterion, optimizer = get_model\n    if get_data is not None:\n        train_loader = get_data()\n\n    for epoch in range(10):\n        for images, labels in train_loeader:\n            ouputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        # Save nth epoch result\n        checkpoint_dir = tempfile.gettempdir()\n        checkpoint_path = checkpoint_dir + "/model.checkpoint"\n        torch.save(model.state_dict(), checkpoint_path)\n        if distributed_report is not None and checkpoint_func is not None:\n            distributed_report(\n                {"loss": loss.item()},\n                checkpoint=checkpoint_func(checkpoint_dir)\n            )\n\nfrom ray.train.torch import TorchTrainer\nfrom ray.train import ScalingConfig, Checkpoint\n\n# Traning With Ray\nfunc train_func(config):\n    training(\n\tbuild_model(ray.train.torch.prepare_model),\n        build_dataloader(ray.train.torch.prepare_data_loader),\n        ray_train.report,\n        Checkpoint.from_directory\n    )\n\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(num_worker=2, use_gpu=False)\n)\nresult = train.fit()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"\u5b58\u53d6ray-model\u4e4b\u8a13\u7df4\u8cc7\u8a0a",children:"\u5b58\u53d6ray model\u4e4b\u8a13\u7df4\u8cc7\u8a0a"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"\u53c3\u6578"}),(0,t.jsx)(e.th,{children:"\u7bc4\u4f8b"}),(0,t.jsx)(e.th,{children:"\u529f\u80fd"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"metrics"}),(0,t.jsx)(e.td,{children:"result.metrics"}),(0,t.jsx)(e.td,{children:"\u8a13\u7df4\u6307\u6a19\u56de\u5831"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"checkpoint"}),(0,t.jsx)(e.td,{children:"result.checkpoint"}),(0,t.jsx)(e.td,{children:"\u6700\u65b0\u4e00\u6b21\u7684\u8a13\u7df4\u56de\u5831"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"path"}),(0,t.jsx)(e.td,{children:"result.path"}),(0,t.jsx)(e.td,{children:"\u5b58log\u7684\u8def\u5f91"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:"error"}),(0,t.jsx)(e.td,{children:"result.error"}),(0,t.jsx)(e.td,{children:"\u8a13\u7df4\u5931\u6557\u8cc7\u8a0a\u56de\u5831"})]})]})]}),"\n",(0,t.jsx)(e.h2,{id:"pytorch-lightning",children:"Pytorch lightning"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\n\nimport pytorch_lightning as pl\n\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\nclass ImageClassifier(pl.LightningModule):\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n\tself.model = resnet18(num_classes=10)\n\tself.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n\tself.criterion = CrossEntropyLoss())\n\n    def forward(self, x):\n\treturn self.model(x)\n\n    def train_step(self, batch, batch_idx):\n\tx, y = batch\n\toutputs = self.forward(x)\n\tloss = self.criterion(outputs, y)\n\tself.log("loss", loss, on_step=True, prog_bar=True)\n\treturn loss\n\n    def configure_optimizer(self):\n\treturn Adam(self.model.parameters(), lr=0.001)\n\nfunc build_dataloader(distributed_loader=None):\n    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n    train_data = FashionMNIST(\'./data\', train=True, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n    if distributed_loader is not None:\n        train_loader = distributed_loader(train_loader)\n    return train_loader\n\ndef train_func(config):\n    model = ImageClassifier()\n\n    # Add some parameters in pl.Trainer\n    # Parameters include devices, accelerator, strategy, plugins and callbacks\n    trainer = pl.Trainer(\n        max_epochs = 10,\n\tdevices="auto",\n\taccelerator="auto",\n\tstrategy=ray.train.lightning.RayDDPStrategy(),\n\tplugins=[ray.train.lightning.RayLightningEnvironment()],\n\tcallbacks=[ray.train.lightning.RayTrainRportCallback()],\n    )\n\n    trainer = ray.train.lightning.prepare_trainer(trainer)\n    trainer.fit(model, train_dataloaders=build_dataloader())\n\ntrainer =TorchTrainer(\n        train_func,\n\tScalingConfig(num_worker=2, use_gpu=True),\n    )\nresult = trainer.fit()\n\n\n'})}),"\n",(0,t.jsx)(e.h2,{id:"hugging-face-transformers",children:"Hugging Face Transformers"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n)\n\n# Encapsulate the data processing and prepare the basic huggingface model.\ndef general_train():\n    dataset = load_dataset("yelp_review_full")\n    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")\n\n    def tokenize_function(examples):\n        return tokenizer(examples["text"], padding="max_length", truncation=True)\n\n    small_train_dataset = dataset["train"].select(range(1000)).map(\n        tokenize_function,\n        batch=True,\n    )\n    small_eval_dataset = dataset["test"].select(range(1000)).map(\n        tokenize_function,\n        batch=True,\n    )\n\n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        "bert-base-cased",\n        num_labels=5\n    )\n\n    # Metrics\n    metric = evaluate.load("accuracy")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predications = np.argmax(logits, axis=-1)\n        return metric.compute(\n\t        predictions=predictions,\n\t        references=labels\n        )\n\n    training_args = TrainingArguments(\n        output_dir="test_trainer",\n        evaluation_strategy="epoch",\n        report_to="none"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=trainging_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\nimport ray.train.huggingface.transformers\nfrom ray.train. import ScalingConfig\nfrom ray.train.torch import TorchTrainer\n\n# Encapsulate the training and the evaluation after the model fits the ray config.\ndef train_func(config):\n    trainer = general_train()\n    trainer.add_callback(\n        ray.train.huggingface..transformers.RayTrainReportCallback(),\n    )\n    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n\n    trainer.train()\n\nray_trainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(\n        num_worker=4,\n        use_gpu=True,\n    )\n)\nray_trainer.fit()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"hugging-face-accelerate",children:"Hugging Face Accelerate"}),"\n",(0,t.jsx)(e.h2,{id:"deepspeed",children:"DeepSpeed"}),"\n",(0,t.jsx)(e.h2,{id:"tensorflow---keras",children:"Tensorflow &  Keras"}),"\n",(0,t.jsx)(e.h2,{id:"xgboost--lightgbm",children:"XGBoost & LightGBM"}),"\n",(0,t.jsx)(e.h2,{id:"horovod",children:"Horovod"})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>s});var t=r(6540);const a={},i=t.createContext(a);function o(n){const e=t.useContext(i);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);