"use strict";(self.webpackChunk_0yukali0=self.webpackChunk_0yukali0||[]).push([[782],{3905:(e,r,n)=>{n.d(r,{Zo:()=>d,kt:()=>m});var t=n(7294);function a(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function o(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function i(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?o(Object(n),!0).forEach((function(r){a(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function c(e,r){if(null==e)return{};var n,t,a=function(e,r){if(null==e)return{};var n,t,a={},o=Object.keys(e);for(t=0;t<o.length;t++)n=o[t],r.indexOf(n)>=0||(a[n]=e[n]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)n=o[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=t.createContext({}),s=function(e){var r=t.useContext(l),n=r;return e&&(n="function"==typeof e?e(r):i(i({},r),e)),n},d=function(e){var r=s(e.components);return t.createElement(l.Provider,{value:r},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},u=t.forwardRef((function(e,r){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),p=s(n),u=a,m=p["".concat(l,".").concat(u)]||p[u]||f[u]||o;return n?t.createElement(m,i(i({ref:r},d),{},{components:n})):t.createElement(m,i({ref:r},d))}));function m(e,r){var n=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=u;var c={};for(var l in r)hasOwnProperty.call(r,l)&&(c[l]=r[l]);c.originalType=e,c[p]="string"==typeof e?e:a,i[1]=c;for(var s=2;s<o;s++)i[s]=n[s];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}u.displayName="MDXCreateElement"},6879:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>i,default:()=>f,frontMatter:()=>o,metadata:()=>c,toc:()=>s});var t=n(7462),a=(n(7294),n(3905));const o={id:"cancerwithray",title:"Ray\u5206\u6563\u5f0f\u8a13\u7df4-\u764c\u75c7\u7d30\u80de\u8a3a\u65b7"},i="Cancer",c={unversionedId:"AI/framework/ray/train/kaggle/cancerwithray",id:"AI/framework/ray/train/kaggle/cancerwithray",title:"Ray\u5206\u6563\u5f0f\u8a13\u7df4-\u764c\u75c7\u7d30\u80de\u8a3a\u65b7",description:"Kaggle URL",source:"@site/docs/AI/framework/ray/train/kaggle/cancerwithray.md",sourceDirName:"AI/framework/ray/train/kaggle",slug:"/AI/framework/ray/train/kaggle/cancerwithray",permalink:"/docs/AI/framework/ray/train/kaggle/cancerwithray",draft:!1,editUrl:"https://github.com/0yukali0/0yukali0.github.io/docs/AI/framework/ray/train/kaggle/cancerwithray.md",tags:[],version:"current",frontMatter:{id:"cancerwithray",title:"Ray\u5206\u6563\u5f0f\u8a13\u7df4-\u764c\u75c7\u7d30\u80de\u8a3a\u65b7"},sidebar:"tutorialSidebar",next:{title:"Ray\u5206\u6563\u5f0f\u8a13\u7df4-DataScienceLondon",permalink:"/docs/AI/framework/ray/train/kaggle/datasciencelondonwithray"}},l={},s=[{value:"Kaggle URL",id:"kaggle-url",level:2},{value:"Ray\u7a0b\u5f0f\u78bc",id:"ray\u7a0b\u5f0f\u78bc",level:2}],d={toc:s},p="wrapper";function f(e){let{components:r,...n}=e;return(0,a.kt)(p,(0,t.Z)({},d,n,{components:r,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"cancer"},"Cancer"),(0,a.kt)("h2",{id:"kaggle-url"},"Kaggle URL"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},(0,a.kt)("a",{parentName:"em",href:"https://www.kaggle.com/datasets/erdemtaha/cancer-data/data"},"https://www.kaggle.com/datasets/erdemtaha/cancer-data/data")))),(0,a.kt)("h2",{id:"ray\u7a0b\u5f0f\u78bc"},"Ray\u7a0b\u5f0f\u78bc"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport pandas as pd\n\nclass CustomDataset(Dataset):\n    def __init__(self, x, y):\n        self.dataset = x\n        self.labels = y\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        x = self.dataset.iloc[idx]\n        y = self.labels.iloc[idx]\n        return torch.tensor(x.values).float(), torch.tensor([y]).float()\n\ndef CreateDataset(path="/home/user/camcer/Cancer_Data.csv"):\n    df = pd.read_csv(path)\n    df = df.loc[:, ~df.columns.str.contains(\'^Unnamed\')]\n    x = df.iloc[: ,2:].astype("float")\n    df["diagnosis"] = df["diagnosis"].apply(lambda x: 1 if x == \'M\' else 0)\n    y = df["diagnosis"].astype("int")\n    dataset = CustomDataset(x=x, y=y)\n    return dataset\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(30, 40),\n            nn.ReLU(),\n            nn.Linear(40, 40),\n            nn.ReLU(),\n            nn.Linear(40, 40),\n            nn.ReLU(),\n            nn.Linear(40, 1),\n            nn.Sigmoid(),\n        )\n    def forward(self, x):\n        return self.linear_relu_stack(x)\n\ndef train(epoch, device, dataloader, model, criterion, optimizer):\n    for epoch in range(epoch):\n        for X, y in dataloader:\n            model.train()\n            optimizer.zero_grad()\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n        metrics = {"loss": loss.item(), "epoch": epoch}\n        if ray.train.get_context().get_world_rank() == 0:\n            print(metrics)\n\nimport ray.train.torch\ndef train_func(config):\n    batch_size = 64\n    device = (\n        "cuda"\n        if torch.cuda.is_available()\n        else "cpu"\n    )\n    model = NeuralNetwork().to(device)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n    dataset = CreateDataset()\n    dataloader = DataLoader(dataset, batch_size)\n\n    model = ray.train.torch.prepare_model(model)\n    dataloader = ray.train.torch.prepare_data_loader(dataloader)\n\n    train(10, device, dataloader, model, criterion, optimizer)\n\nscaling_config = ray.train.ScalingConfig(num_workers=4, use_gpu=False)\ntrainer = ray.train.torch.TorchTrainer(\n    train_func,\n    scaling_config=scaling_config,\n)\nresult = trainer.fit()\n')))}f.isMDXComponent=!0}}]);