"use strict";(self.webpackChunk_0yukali0=self.webpackChunk_0yukali0||[]).push([[101],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),d=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=d(e.components);return r.createElement(s.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(n),f=a,m=p["".concat(s,".").concat(f)]||p[f]||u[f]||o;return n?r.createElement(m,i(i({ref:t},c),{},{components:n})):r.createElement(m,i({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:a,i[1]=l;for(var d=2;d<o;d++)i[d]=n[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},974:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var r=n(7462),a=(n(7294),n(3905));const o={id:"datasciencelondon",title:"DataScienceLondon"},i="DataScienceLondon",l={unversionedId:"pytorch/kaggle/datasciencelondon",id:"pytorch/kaggle/datasciencelondon",title:"DataScienceLondon",description:"\u8cc7\u6599\u4f86\u6e90",source:"@site/docs/pytorch/kaggle/dataScienceLondon.md",sourceDirName:"pytorch/kaggle",slug:"/pytorch/kaggle/datasciencelondon",permalink:"/docs/pytorch/kaggle/datasciencelondon",draft:!1,editUrl:"https://github.com/0yukali0/0yukali0.github.io/docs/pytorch/kaggle/dataScienceLondon.md",tags:[],version:"current",frontMatter:{id:"datasciencelondon",title:"DataScienceLondon"},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/docs/intro"},next:{title:"mnist",permalink:"/docs/pytorch/kaggle/mnist"}},s={},d=[{value:"\u8cc7\u6599\u4f86\u6e90",id:"\u8cc7\u6599\u4f86\u6e90",level:2},{value:"Pytorch\u7a0b\u5f0f",id:"pytorch\u7a0b\u5f0f",level:2}],c={toc:d},p="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"datasciencelondon"},"DataScienceLondon"),(0,a.kt)("h2",{id:"\u8cc7\u6599\u4f86\u6e90"},"\u8cc7\u6599\u4f86\u6e90"),(0,a.kt)("h2",{id:"pytorch\u7a0b\u5f0f"},"Pytorch\u7a0b\u5f0f"),(0,a.kt)("p",null,"'''python\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor"),(0,a.kt)("h1",{id:"download-training-data-from-open-datasets"},"Download training data from open datasets."),(0,a.kt)("p",null,'training_data = datasets.FashionMNIST(\nroot="data",\ntrain=True,\ndownload=True,\ntransform=ToTensor(),\n)'),(0,a.kt)("h1",{id:"download-test-data-from-open-datasets"},"Download test data from open datasets."),(0,a.kt)("p",null,'test_data = datasets.FashionMNIST(\nroot="data",\ntrain=False,\ndownload=True,\ntransform=ToTensor(),\n)'),(0,a.kt)("p",null,"batch_size = 64"),(0,a.kt)("h1",{id:"create-data-loaders"},"Create data loaders."),(0,a.kt)("p",null,"train_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)"),(0,a.kt)("p",null,'for X, y in test_dataloader:\nprint(f"Shape of X ',"[N, C, H, W]",': {X.shape}")\nprint(f"Shape of y: {y.shape} {y.dtype}")\nbreak'),(0,a.kt)("h1",{id:"get-cpu-gpu-or-mps-device-for-training"},"Get cpu, gpu or mps device for training."),(0,a.kt)("p",null,'device = (\n"cuda"\nif torch.cuda.is_available()\nelse "mps"\nif torch.backends.mps.is_available()\nelse "cpu"\n)\nprint(f"Using {device} device")'),(0,a.kt)("h1",{id:"define-model"},"Define model"),(0,a.kt)("p",null,"class NeuralNetwork(nn.Module):\ndef ",(0,a.kt)("strong",{parentName:"p"},"init"),"(self):\nsuper().",(0,a.kt)("strong",{parentName:"p"},"init"),"()\nself.flatten = nn.Flatten()\nself.linear_relu_stack = nn.Sequential(\nnn.Linear(28*28, 512),\nnn.ReLU(),\nnn.Linear(512, 512),\nnn.ReLU(),\nnn.Linear(512, 10)\n)\ndef forward(self, x):\nx = self.flatten(x)\nlogits = self.linear_relu_stack(x)\nreturn logits"),(0,a.kt)("p",null,"model = NeuralNetwork().to(device)\nprint(model)"),(0,a.kt)("p",null,"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"),(0,a.kt)("p",null,"def train(dataloader, model, loss_fn, optimizer):\nsize = len(dataloader.dataset)\nmodel.train()\nfor batch, (X, y) in enumerate(dataloader):\nX, y = X.to(device), y.to(device)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'    # Compute prediction error\n    pred = model(X)\n    #print(len(pred), len(y))\n    loss = loss_fn(pred, y)\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    if batch % 100 == 0:\n        loss, current = loss.item(), (batch + 1) * len(X)\n        print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")\n')),(0,a.kt)("p",null,'def test(dataloader, model, loss_fn):\nsize = len(dataloader.dataset)\nnum_batches = len(dataloader)\nmodel.eval()\ntest_loss, correct = 0, 0\nwith torch.no_grad():\nfor X, y in dataloader:\nX, y = X.to(device), y.to(device)\npred = model(X)\n#print(len(pred), len(y))\ntest_loss += loss_fn(pred, y).item()\ncorrect += (pred.argmax(1) == y).type(torch.float).sum().item()\ntest_loss /= num_batches\ncorrect /= size\nprint(f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n")'),(0,a.kt)("p",null,'epochs = 5\nfor t in range(epochs):\nprint(f"Epoch {t+1}\\n-------------------------------")\ntrain(train_dataloader, model, loss_fn, optimizer)\ntest(test_dataloader, model, loss_fn)'),(0,a.kt)("p",null,"print(model(test_dataloader.dataset.iloc","[0]","))\n'''"))}u.isMDXComponent=!0}}]);