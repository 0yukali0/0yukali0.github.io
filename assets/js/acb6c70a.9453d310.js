"use strict";(self.webpackChunk_0yukali0=self.webpackChunk_0yukali0||[]).push([[751],{3905:(e,t,r)=>{r.d(t,{Zo:()=>d,kt:()=>b});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function l(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?l(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):l(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function o(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},l=Object.keys(e);for(n=0;n<l.length;n++)r=l[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)r=l[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=n.createContext({}),p=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),c=p(r),u=a,b=c["".concat(s,".").concat(u)]||c[u]||m[u]||l;return r?n.createElement(b,i(i({ref:t},d),{},{components:r})):n.createElement(b,i({ref:t},d))}));function b(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var l=r.length,i=new Array(l);i[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[c]="string"==typeof e?e:a,i[1]=o;for(var p=2;p<l;p++)i[p]=r[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}u.displayName="MDXCreateElement"},109:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=r(7462),a=(r(7294),r(3905));const l={id:"mllib-linear",title:"linear"},i=void 0,o={unversionedId:"spark/spark_mllib/mllib-linear",id:"spark/spark_mllib/mllib-linear",title:"linear",description:"SVM",source:"@site/docs/spark/spark_mllib/mllib-linear.md",sourceDirName:"spark/spark_mllib",slug:"/spark/spark_mllib/mllib-linear",permalink:"/blog/2024-02-13-info-blog-post/docs/spark/spark_mllib/mllib-linear",draft:!1,editUrl:"https://github.com/0yukali0/0yukali0.github.io/docs/spark/spark_mllib/mllib-linear.md",tags:[],version:"current",frontMatter:{id:"mllib-linear",title:"linear"},sidebar:"tutorialSidebar",previous:{title:"pyspark",permalink:"/blog/2024-02-13-info-blog-post/docs/spark/pyspark"},next:{title:"zookeeper\u53e2\u96c6\u5efa\u7acb",permalink:"/blog/2024-02-13-info-blog-post/docs/zookeeper/buildzookeeper"}},s={},p=[{value:"SVM",id:"svm",level:2},{value:"LogisticRegression",id:"logisticregression",level:2}],d={toc:p},c="wrapper";function m(e){let{components:t,...r}=e;return(0,a.kt)(c,(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"svm"},"SVM"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.mllib.classification import SVMWithSGD, SVMModel\nfrom pyspark.mllib.regression import LabeledPoint\n\n# Load and parse the data\ndef parsePoint(line):\n    values = [float(x) for x in line.split(\' \')]\n    return LabeledPoint(values[0], values[1:])\n\ndata = sc.textFile("data/mllib/sample_svm_data.txt")\nparsedData = data.map(parsePoint)\n\n# Build the model\nmodel = SVMWithSGD.train(parsedData, iterations=100)\n\n# Evaluating the model on training data\nlabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\ntrainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\nprint("Training Error = " + str(trainErr))\n\n# Save and load model\nmodel.save(sc, "target/tmp/pythonSVMWithSGDModel")\nsameModel = SVMModel.load(sc, "target/tmp/pythonSVMWithSGDModel")\n')),(0,a.kt)("h2",{id:"logisticregression"},"LogisticRegression"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint\n\n\n# Build the model\nmodel = LogisticRegressionWithLBFGS.train(parsedData)\n\n# Evaluating the model on training data\nlabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\ntrainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\nprint("Training Error = " + str(trainErr))\n\n# Save and load model\nmodel.save(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")\nsameModel = LogisticRegressionModel.load(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")\n')))}m.isMDXComponent=!0}}]);